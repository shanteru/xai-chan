{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('/home/student/Desktop/31171109-donotdelete/xai-chan/result/imagenet/_Fold_4_5_400X_BreakHis_FT_60_resnet50_imagenet_/_75_85.25798525798525_86.29455539952498_0.8421162962913513.pth'), PosixPath('/home/student/Desktop/31171109-donotdelete/xai-chan/result/imagenet/_Fold_1_5_400X_BreakHis_FT_60_resnet50_imagenet_/_68_91.83168316831683_89.45802206500767_0.9190493822097778.pth'), PosixPath('/home/student/Desktop/31171109-donotdelete/xai-chan/result/imagenet/_Fold_0_5_400X_BreakHis_FT_60_resnet50_imagenet_/_55_89.33717579250721_86.90818840905345_0.893511176109314.pth'), PosixPath('/home/student/Desktop/31171109-donotdelete/xai-chan/result/imagenet/_Fold_2_5_400X_BreakHis_FT_60_resnet50_imagenet_/_37_96.07250755287009_95.06917631917632_0.9614846110343933.pth'), PosixPath('/home/student/Desktop/31171109-donotdelete/xai-chan/result/imagenet/_Fold_3_5_400X_BreakHis_FT_60_resnet50_imagenet_/_9_86.70694864048339_88.22885510136258_0.8655245304107666.pth')]\n",
      "Best weight: /home/student/Desktop/31171109-donotdelete/xai-chan/result/imagenet/_Fold_4_5_400X_BreakHis_FT_60_resnet50_imagenet_/_75_85.25798525798525_86.29455539952498_0.8421162962913513.pth\n"
     ]
    }
   ],
   "source": [
    "import os,yaml\n",
    "from pathlib import Path\n",
    "def load_config(config_path):\n",
    "    \"\"\"Load configuration information from a yaml file.\"\"\"\n",
    "    with open(config_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "\n",
    "def find_best_weights_for_magnification(root_dir, magnification, num_folds=5):\n",
    "    \"\"\"\n",
    "    Iterate through each fold and find the best weight file based on validation accuracy.\n",
    "\n",
    "    Args:\n",
    "    - root_dir (str): Root directory containing fold sub-directories.\n",
    "    - magnification (str): Desired magnification like \"400X\".\n",
    "    - num_folds (int): Number of folds. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "    - str: Path to the best weight file across all folds.\n",
    "    \"\"\"\n",
    "    \n",
    "    bc_config = load_config(\"run_imagenet.yaml\")\n",
    "    weight_files = []\n",
    "    \n",
    "    for fold in list(bc_config[\"computational_infra\"][\"fold_to_gpu_mapping\"].keys()):\n",
    "        fold_dir = Path(root_dir) / f\"_{fold}_{magnification}_BreakHis_FT_60_resnet50_imagenet_\"\n",
    "        # Gather all weight files for this fold\n",
    "        fold_weight_files = list(fold_dir.glob('*.pth'))\n",
    "        weight_files.extend(fold_weight_files)\n",
    "        \n",
    "    # Sort the weight files based on validation accuracy which is the second value in the filename\n",
    "    sorted_files = sorted(weight_files, key=lambda x: float(x.stem.split('_')[1]), reverse=True)\n",
    "    print(sorted_files)\n",
    "    \n",
    "    # Return the path of the weight file with the highest validation accuracy\n",
    "    return str(sorted_files[0]) if sorted_files else None\n",
    "\n",
    "# Example usage\n",
    "root_directory = \"/home/student/Desktop/31171109-donotdelete/xai-chan/result/imagenet\"\n",
    "magnification = \"400X\"\n",
    "best_weight = find_best_weights_for_magnification(root_directory, magnification)\n",
    "print('Best weight:', best_weight)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test_set(self, test_loader):\n",
    "    confusion_matrix_test = torch.zeros(len(bc_config.binary_label_list), len(bc_config.binary_label_list))\n",
    "    self.model.eval()\n",
    "    with torch.no_grad():\n",
    "        for patient_id, magnification, item_dict, binary_label, multi_label in tqdm(test_loader):\n",
    "            view = item_dict[magnification[0]]\n",
    "            view = view.cuda(self.device, non_blocking=True)                \n",
    "            target = binary_label.to(self.device)\n",
    "            outputs = self.model(view)\n",
    "            outputs = outputs.squeeze(1)\n",
    "            target = target.type_as(outputs)\n",
    "            \n",
    "            # Since it's testing, no need for loss calculation\n",
    "            predicted = (outputs > self.threshold).int()\n",
    "            \n",
    "            for targetx, predictedx in zip(target.view(-1), predicted.view(-1)):\n",
    "                confusion_matrix_test[(targetx.long(), predictedx.long())] += 1\n",
    "\n",
    "    # Extract metrics from the confusion matrix similar to your validation function\n",
    "    weighted_f1, accuracy, _, classwise_precision, classwise_recall, classwise_f1 = self.get_metrics_from_confusion_matrix(confusion_matrix_test)\n",
    "    print(f'{self.experiment_description}: Test classwise precision', classwise_precision)\n",
    "    print(f'{self.experiment_description}: Test classwise recall', classwise_recall)\n",
    "    print(f'{self.experiment_description}: Test classwise f1', classwise_f1)\n",
    "    print(f'{self.experiment_description}: Test Weighted F1', weighted_f1)\n",
    "    print(f'{self.experiment_description}: Test Accuracy', accuracy)\n",
    "    print(confusion_matrix_test)\n",
    "    return (weighted_f1, accuracy, classwise_precision, classwise_recall, classwise_f1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Explainations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "# Append custom paths to sys.path for importing custom modules\n",
    "sys.path.append(os.path.dirname(\"/home/student/Desktop/31171109-donotdelete/xai-chan/utils\"))\n",
    "from utils import train_utils, dataset_test, transform, models\n",
    "# Append custom paths to sys.path for importing custom modules\n",
    "sys.path.append(os.path.dirname(\"/home/student/Desktop/31171109-donotdelete/xai-chan/utils\"))\n",
    "\n",
    "# Initialize device\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "452 445 422 391\n",
      "391\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "test_loader = dataset_test.get_breakhis_data_loader(\n",
    "        dataset_path=\"/home/student/Desktop/31171109-donotdelete/xai-chan/dataset/Fold_0_5/val_20\",\n",
    "        transform=transform.resize_transform,\n",
    "        pre_processing=[],\n",
    "        image_type_list=[\"400X\"],\n",
    "        num_workers=2,\n",
    "        is_test=True\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/anaconda3/envs/xai-chan/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/student/anaconda3/envs/xai-chan/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet_Model(\n",
       "  (model): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Sequential(\n",
       "      (0): Dropout(p=0.8, inplace=False)\n",
       "      (1): Linear(in_features=2048, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "version = 50\n",
    "model = models.ResNet_Model(version=version).to(device)\n",
    "weights_path = \"/home/student/Desktop/31171109-donotdelete/xai-chan/result/imagenet/_Fold_2_5_400X_BreakHis_FT_60_resnet50_imagenet_/_37_96.07250755287009_95.06917631917632_0.9614846110343933.pth\"  # TODO: Provide the model path\n",
    "model.load_state_dict(torch.load(weights_path, map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:04<00:00,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test classwise precision: [0.9097744 0.996124 ]\n",
      "Test classwise recall: [0.9918033  0.95539033]\n",
      "Test classwise f1: [0.9490196 0.9753321]\n",
      "Test Weighted F1: 0.9671220183372498\n",
      "Test Accuracy: 96.67519181585678\n",
      "Confusion Matrix:\n",
      "[[121.   1.]\n",
      " [ 12. 257.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def get_metrics_from_confusion_matrix(confusion_matrix_epoch):\n",
    "        epoch_classwise_precision_manual_cpu = np.array(confusion_matrix_epoch.diag().cpu()) / np.array(confusion_matrix_epoch.cpu()).sum(axis=0)\n",
    "        epoch_classwise_precision_manual_cpu = np.nan_to_num(epoch_classwise_precision_manual_cpu, nan=0, neginf=0, posinf=0)\n",
    "        epoch_classwise_recall_manual_cpu = np.array(confusion_matrix_epoch.diag().cpu()) / np.array(confusion_matrix_epoch.cpu()).sum(axis=1)\n",
    "        epoch_classwise_recall_manual_cpu = np.nan_to_num(epoch_classwise_recall_manual_cpu, nan=0, neginf=0, posinf=0)\n",
    "        epoch_classwise_f1_manual_cpu = 2 * (epoch_classwise_precision_manual_cpu * epoch_classwise_recall_manual_cpu) / (epoch_classwise_precision_manual_cpu + epoch_classwise_recall_manual_cpu)\n",
    "        epoch_classwise_f1_manual_cpu = np.nan_to_num(epoch_classwise_f1_manual_cpu, nan=0, neginf=0, posinf=0)\n",
    "        epoch_avg_f1_manual = np.sum(epoch_classwise_f1_manual_cpu * np.array(confusion_matrix_epoch.cpu()).sum(axis=1)) / np.array(confusion_matrix_epoch.cpu()).sum(axis=1).sum()\n",
    "        epoch_acc_manual = 100 * np.sum(np.array(confusion_matrix_epoch.diag().cpu())) / np.sum(np.array(confusion_matrix_epoch.cpu()))\n",
    "        return (\n",
    "         epoch_avg_f1_manual, epoch_acc_manual, epoch_classwise_precision_manual_cpu, epoch_classwise_recall_manual_cpu, epoch_classwise_f1_manual_cpu)\n",
    "\n",
    "confusion_matrix_test = torch.zeros(2, 2).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for patient_id, magnification, item_dict, binary_label, multi_label in tqdm(test_loader):\n",
    "        view = item_dict[magnification[0]]\n",
    "        view = view.cuda(device, non_blocking=True)                \n",
    "        target = binary_label.to(device)\n",
    "        outputs = model(view)\n",
    "        outputs = outputs.squeeze(1)\n",
    "        target = target.type_as(outputs)\n",
    "        \n",
    "        # Since it's testing, no need for loss calculation\n",
    "        predicted = (outputs > 0.2).int()\n",
    "        \n",
    "        for targetx, predictedx in zip(target.view(-1), predicted.view(-1)):\n",
    "            confusion_matrix_test[(targetx.long(), predictedx.long())] += 1\n",
    "\n",
    "# Extract metrics from the confusion matrix similar to your validation function\n",
    "\n",
    "weighted_f1, accuracy, classwise_precision, classwise_recall, classwise_f1 = get_metrics_from_confusion_matrix(confusion_matrix_test.cpu())\n",
    "# Display the metrics\n",
    "print(f'Test classwise precision: {classwise_precision}')\n",
    "print(f'Test classwise recall: {classwise_recall}')\n",
    "print(f'Test classwise f1: {classwise_f1}')\n",
    "print(f'Test Weighted F1: {weighted_f1}')\n",
    "print(f'Test Accuracy: {accuracy}')\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix_test.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision.transforms import ToPILImage\n",
    "from zennit.attribution import Gradient, SmoothGrad\n",
    "from zennit.core import Stabilizer\n",
    "from zennit.composites import EpsilonGammaBox, EpsilonPlusFlat\n",
    "from zennit.composites import SpecialFirstLayerMapComposite, NameMapComposite\n",
    "from zennit.rules import Epsilon, ZPlus, ZBox, Norm, Pass, Flat\n",
    "from zennit.types import Convolution, Activation, AvgPool, Linear as AnyLinear\n",
    "from zennit.types import BatchNorm, MaxPool\n",
    "from zennit.torchvision import VGGCanonizer, ResNetCanonizer\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define XAI composite\n",
    "low, high = torch.tensor([[[[[0.]]] * 3], [[[[1.]]] * 3]])\n",
    "composite = SpecialFirstLayerMapComposite(\n",
    "    layer_map=[\n",
    "        (nn.ReLU, Pass()),\n",
    "        (nn.AvgPool2d, Norm()),\n",
    "        (nn.Conv2d, ZPlus()),\n",
    "        (nn.Linear, Epsilon(epsilon=1e-6)),\n",
    "        (nn.BatchNorm2d, Pass()),\n",
    "    ],\n",
    "    first_map=[\n",
    "        (AnyLinear, ZBox(low, high))\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zennit.image import imgify, imsave\n",
    "from zennit.torchvision import ResNetCanonizer\n",
    "from zennit.composites import EpsilonPlusFlat\n",
    "from zennit.attribution import Gradient\n",
    "\n",
    "# Use the ResNet-specific canonizer\n",
    "canonizer = ResNetCanonizer()\n",
    "\n",
    "# Create a composite, specifying the canonizers\n",
    "composite = EpsilonPlusFlat(canonizers=[canonizer])\n",
    "\n",
    "# Iterate over the test_loader again\n",
    "with torch.no_grad():\n",
    "    for patient_id, magnification, item_dict, binary_label, multi_label in tqdm(test_loader):\n",
    "\n",
    "        view = item_dict[magnification[0]]\n",
    "        view = view.cuda(device, non_blocking=True)                \n",
    "\n",
    "        outputs = model(view)\n",
    "        outputs = outputs.squeeze(1)\n",
    "        \n",
    "\n",
    "        \n",
    "        print(view)\n",
    "        # Since it's testing, no need for loss calculation\n",
    "        predicted = (outputs > 0.2).int()\n",
    "        print(predicted)\n",
    "         # Iterate over each image and its corresponding prediction\n",
    "        for i, (single_view, single_pred) in enumerate(zip(view, predicted)):\n",
    "            \n",
    "            with Gradient(model=model, composite=composite) as attributor:\n",
    "                _, attribution = attributor(single_view.unsqueeze(0), single_pred.unsqueeze(0))\n",
    "\n",
    "            relevance = attribution.sum(1).cpu()\n",
    "            imsave(f\"/home/student/Desktop/31171109-donotdelete/xai-chan/saved/explanation/{patient_id}_{magnification[0]}_{[i]}.png\", relevance, symmetric=True, cmap='coldnhot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import display\n",
    "\n",
    "# Make sure your model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define canonizer and composite\n",
    "canonizer = ResNetCanonizer()\n",
    "composite = EpsilonPlusFlat(canonizers=[canonizer])\n",
    "\n",
    "# Iterate over the test_loader again\n",
    "with torch.no_grad():\n",
    "    for patient_id, magnification, item_dict, binary_label, multi_label in tqdm(test_loader):\n",
    "\n",
    "        view = item_dict[magnification[0]]\n",
    "        view = view.cuda(device, non_blocking=True)                \n",
    "\n",
    "        outputs = model(view)\n",
    "        outputs = outputs.squeeze(1)\n",
    "        \n",
    "\n",
    "        \n",
    "        print(view)\n",
    "        # Since it's testing, no need for loss calculation\n",
    "        predicted = (outputs > 0.2).int()\n",
    "        print(predicted)\n",
    "   \n",
    "        \n",
    "        \n",
    "        # Get the XAI attribution\n",
    "        with Gradient(model=model, composite=composite) as attributor:\n",
    "            _, attribution = attributor(view, predicted)\n",
    "\n",
    "        # Sum over the channels and visualize\n",
    "        relevance = attribution.sum(1).cpu()\n",
    "        imsave(f\"/home/student/Desktop/31171109-donotdelete/xai-chan/saved/explanation/{patient_id}_{magnification[0]}.png\", relevance, symmetric=True, cmap='coldnhot')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image: SOB_B_F-14-21998EF-400-020.png\n",
      "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=700x460 at 0x7F76BBF61400>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "pic should be Tensor or ndarray. Got <class 'PIL.PngImagePlugin.PngImageFile'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m             imsave(heatmap_path, relevance, symmetric\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, cmap\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcoldnhot\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[39m# Execute the function\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m generate_heatmaps(\u001b[39m\"\u001b[39;49m\u001b[39m/home/student/Desktop/31171109-donotdelete/xai-chan/explanation/val_10/SOB_B_F_14-21998EF/400X\u001b[39;49m\u001b[39m\"\u001b[39;49m, model)\n",
      "Cell \u001b[0;32mIn[57], line 37\u001b[0m, in \u001b[0;36mgenerate_heatmaps\u001b[0;34m(directory, model)\u001b[0m\n\u001b[1;32m     35\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(img_path)\n\u001b[1;32m     36\u001b[0m \u001b[39mprint\u001b[39m(img)\n\u001b[0;32m---> 37\u001b[0m img_tensor \u001b[39m=\u001b[39m transform\u001b[39m.\u001b[39;49mresize_transform(img)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(device)  \u001b[39m# Add batch dimension and move to device\u001b[39;00m\n\u001b[1;32m     39\u001b[0m relevance \u001b[39m=\u001b[39m compute_heatmap(img_tensor, model, target)\n\u001b[1;32m     41\u001b[0m \u001b[39m# Save the heatmap\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/xai-chan/lib/python3.9/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/envs/xai-chan/lib/python3.9/site-packages/torchvision/transforms/transforms.py:234\u001b[0m, in \u001b[0;36mToPILImage.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[1;32m    226\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[39m        pic (Tensor or numpy.ndarray): Image to be converted to PIL Image.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m \n\u001b[1;32m    233\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_pil_image(pic, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode)\n",
      "File \u001b[0;32m~/anaconda3/envs/xai-chan/lib/python3.9/site-packages/torchvision/transforms/functional.py:262\u001b[0m, in \u001b[0;36mto_pil_image\u001b[0;34m(pic, mode)\u001b[0m\n\u001b[1;32m    259\u001b[0m     _log_api_usage_once(to_pil_image)\n\u001b[1;32m    261\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(pic, torch\u001b[39m.\u001b[39mTensor) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(pic, np\u001b[39m.\u001b[39mndarray)):\n\u001b[0;32m--> 262\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpic should be Tensor or ndarray. Got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(pic)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    264\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(pic, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    265\u001b[0m     \u001b[39mif\u001b[39;00m pic\u001b[39m.\u001b[39mndimension() \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m {\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m}:\n",
      "\u001b[0;31mTypeError\u001b[0m: pic should be Tensor or ndarray. Got <class 'PIL.PngImagePlugin.PngImageFile'>."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "from zennit.image import imgify, imsave  # For creating visualizations\n",
    "from zennit.torchvision import ResNetCanonizer  # For ResNet-specific canonization\n",
    "from zennit.composites import EpsilonPlusFlat  # For the composite function in LRP\n",
    "from zennit.attribution import Gradient  # For attributing using gradients\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "\n",
    "def compute_heatmap(img_tensor, model, target):\n",
    "    canonizer = ResNetCanonizer()\n",
    "    composite = EpsilonPlusFlat(canonizers=[canonizer])\n",
    "    \n",
    "    with Gradient(model=model, composite=composite) as attributor:\n",
    "        output, attribution = attributor(img_tensor, target)\n",
    "    \n",
    "    # Sum over the channels\n",
    "    relevance = attribution.sum(1).cpu()\n",
    "    return relevance\n",
    "\n",
    "# Function to generate heatmaps for all images in a directory\n",
    "def generate_heatmaps(directory, model):\n",
    "    model.eval()\n",
    "    target = torch.tensor([[1.0]]).to(device)\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "            print(f\"Processing image: {filename}\")\n",
    "\n",
    "            img_path = os.path.join(directory, filename)\n",
    "            img = Image.open(img_path)\n",
    "            print(img)\n",
    "            img_tensor = transform.resize_transform(img).unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "\n",
    "            relevance = compute_heatmap(img_tensor, model, target)\n",
    "            \n",
    "            # Save the heatmap\n",
    "            heatmap_filename = f\"{filename.split('.')[0]}_gradient.png\"  # Removing the original extension and appending the method name\n",
    "            heatmap_path = os.path.join(directory, heatmap_filename)\n",
    "            imsave(heatmap_path, relevance, symmetric=True, cmap='coldnhot')\n",
    "\n",
    "# Execute the function\n",
    "generate_heatmaps(\"/home/student/Desktop/31171109-donotdelete/xai-chan/explanation/val_10/SOB_B_F_14-21998EF/400X\", model)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai-chan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
